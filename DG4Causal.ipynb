{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_j6TSehzmYi6"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "YK8JM_jwsAYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from itertools import chain, combinations\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import scipy.optimize\n",
        "from scipy.stats import f as fdist, ttest_ind, weibull_min\n",
        "from sklearn.linear_model import LinearRegression, PoissonRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import statsmodels.api as sm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import grad"
      ],
      "metadata": {
        "id": "eQhoLrjDkEbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Seeds for reproducibility"
      ],
      "metadata": {
        "id": "XcUk0EXjBLWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 138\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)"
      ],
      "metadata": {
        "id": "GXkK4wfqBB9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretty(vector):\n",
        "    # Convert to tensor if not already\n",
        "    if not isinstance(vector, torch.Tensor):\n",
        "        vector = torch.tensor(vector)\n",
        "    vlist = vector.flatten().tolist()  # Using flatten() instead of view(-1)\n",
        "    return \"[\" + \", \".join(\"{:+.4f}\".format(vi) for vi in vlist) + \"]\""
      ],
      "metadata": {
        "id": "r-FMtVB7kKtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_poisson_nll(input, target, weights):\n",
        "    # Standard Poisson NLL terms\n",
        "    loss = input - target * torch.log(input + 1e-8)\n",
        "    # Multiply each observation's loss by its frequency weight\n",
        "    weighted_loss = (loss * weights).mean()\n",
        "    return weighted_loss"
      ],
      "metadata": {
        "id": "VxxL4VikkPTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Domain Generalisation Algorithms"
      ],
      "metadata": {
        "id": "u8lP66L0qLYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Poisson Deviance Loss Implementation\n",
        "\n",
        "In this cell, we define the `PoissonDevianceLoss` class, a custom loss function implemented using PyTorch. This loss function is specifically designed for Poisson regression tasks, where the observed target values are modeled as Poisson-distributed.\n",
        "\n",
        "#### Key Features:\n",
        "- **Purpose**: Measures the deviance between observed values (`y_true`) and predicted mean values (`y_pred`).\n",
        "- **Implementation Details**:\n",
        "  - Ensures that predicted values are strictly positive (required for Poisson regression).\n",
        "  - Handles edge cases such as `0 * log(0)` safely.\n",
        "  - Implements the standard deviance formula for Poisson regression.\n",
        "\n",
        "#### Parameters:\n",
        "- `y_true`: Observed target values (must be a PyTorch tensor).\n",
        "- `y_pred`: Predicted mean values (must be positive and a PyTorch tensor).\n",
        "\n",
        "#### Returns:\n",
        "- The computed deviance loss as a PyTorch tensor.\n",
        "\n",
        "This function can be used as a loss function in training models for Poisson regression tasks.\n"
      ],
      "metadata": {
        "id": "_j6TSehzmYi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PoissonDevianceLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PoissonDevianceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Compute the deviance loss for Poisson regression.\n",
        "        \"\"\"\n",
        "        if torch.any(y_pred <= 0):\n",
        "            raise ValueError(\"Predicted values must be strictly positive.\")\n",
        "\n",
        "        # Compute deviance components\n",
        "        term1 = y_true * torch.log(y_true / y_pred + 1e-8)  # Add small epsilon to avoid log(0)\n",
        "        term1 = torch.where(y_true > 0, term1, torch.zeros_like(term1))  # Handle 0*log(0) -> 0\n",
        "        term2 = y_true - y_pred\n",
        "\n",
        "        # Deviance formula\n",
        "        deviance = 2 * torch.sum(term1 - term2)\n",
        "        return deviance"
      ],
      "metadata": {
        "id": "pEciZJZBmb7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Invariant Risk Minimization (IRM) Class Implementation\n",
        "\n",
        "In this cell, we define the `InvariantRiskMinimization` class, which implements the IRM algorithm for domain generalization. The objective of IRM is to learn invariant predictors across different environments, minimizing risk while ensuring the invariance of the learned model.\n",
        "\n",
        "#### Key Components:\n",
        "1. **Initialization**:\n",
        "   - Converts the inputs (`exog`, `endog`, and `freqs`) to PyTorch tensors for compatibility.\n",
        "   - Performs hyperparameter tuning by training the model with different regularization values (`reg`), selecting the best model based on validation error.\n",
        "\n",
        "2. **Train Method**:\n",
        "   - Trains the IRM model using a custom loss function incorporating a weighted Poisson Negative Log-Likelihood loss.\n",
        "   - Computes a penalty to ensure the invariance of the gradients across environments.\n",
        "\n",
        "3. **Solution**:\n",
        "   - Provides the learned model parameters as a NumPy array.\n",
        "\n",
        "4. **Prediction**:\n",
        "   - Uses the trained model to predict Poisson rates (`lambda`) for a given input.\n",
        "\n",
        "#### Parameters:\n",
        "- `exog`: Exogenous variables (input features for environments).\n",
        "- `endog`: Endogenous variables (target values for environments).\n",
        "- `freqs`: Weights or frequencies associated with each environment.\n",
        "- `args`: Dictionary of hyperparameters, including learning rate and number of iterations.\n",
        "\n",
        "#### Notable Features:\n",
        "- Includes gradient-based regularization to enforce invariance.\n",
        "- Uses PyTorch for tensor operations, loss computation, and optimization.\n",
        "- Supports weighted Poisson regression for handling environment-specific weights.\n",
        "\n",
        "This class serves as a foundation for training robust predictive models that generalize well across varying environments.\n"
      ],
      "metadata": {
        "id": "1r09hUhMq0D2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InvariantRiskMinimization(object):\n",
        "    def __init__(self, exog, endog, freqs, args):\n",
        "        \"\"\"\n",
        "        Initialize and train the model with the given environments and arguments.\n",
        "\n",
        "        Args:\n",
        "            environments: List of tuples (x, y), where x and y are tensors for each environment.\n",
        "            args: Dictionary of hyperparameters, e.g., learning rate and number of iterations.\n",
        "        \"\"\"\n",
        "        self.best_reg = 0\n",
        "        self.best_err = float(\"inf\")\n",
        "\n",
        "        exog = torch.tensor(np.array(exog), dtype=torch.float32)\n",
        "        endog = torch.tensor(np.array(endog), dtype=torch.float32)\n",
        "        freqs = torch.tensor(np.array(freqs), dtype=torch.float32)\n",
        "\n",
        "        x_val = exog[-1]\n",
        "        y_val = endog[-1]\n",
        "        for reg in [0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]:\n",
        "            self.train(exog[:-1], endog[:-1], freqs[:-1], args, reg=reg)\n",
        "            err = torch.nn.PoissonNLLLoss(log_input=False, reduction='mean')(torch.exp(x_val @ self.solution()), y_val)\n",
        "            #err = torch.mean((torch.exp(x_val @ self.solution()) - y_val) ** 2).item()\n",
        "\n",
        "            if args.verbose:\n",
        "                print(f\"IRM (reg={reg:.3f}) has validation error: {err:.3f}.\")\n",
        "\n",
        "            if err < self.best_err:\n",
        "                self.best_err = err\n",
        "                self.best_reg = reg\n",
        "                self.best_phi = self.phi.clone()\n",
        "\n",
        "        #self.phi = self.best_phi\n",
        "\n",
        "    def train(self, exog, endog, freqs, args, reg=0):\n",
        "        \"\"\"\n",
        "        Train the IRM model using the given environments.\n",
        "\n",
        "        Args:\n",
        "            environments: List of tuples (x, y), where x and y are tensors for each environment.\n",
        "            args: Dictionary of hyperparameters.\n",
        "            reg: Regularization coefficient for IRM penalty.\n",
        "        \"\"\"\n",
        "        dim_x = exog[0].size(1)\n",
        "        self.phi = nn.Parameter(torch.empty(dim_x, dim_x))\n",
        "        nn.init.xavier_uniform_(self.phi)\n",
        "        self.w = nn.Parameter(torch.empty(dim_x, 1))\n",
        "        nn.init.xavier_uniform_(self.w)\n",
        "\n",
        "        optimizer = torch.optim.Adam([self.phi, self.w], lr=args.lr, weight_decay=1e-5)\n",
        "\n",
        "        # Custom weighted Poisson loss function\n",
        "\n",
        "        for iteration in range(args.n_iterations):\n",
        "            total_penalty = 0\n",
        "            total_error = 0\n",
        "\n",
        "            for x_e, y_e, f in zip(exog, endog, freqs):\n",
        "                input = torch.exp(x_e @ self.phi @ self.w)\n",
        "                # Apply weighted loss for this environment\n",
        "                error_e = weighted_poisson_nll(input, y_e, f)\n",
        "                #error_e = nn.PoissonNLLLoss(log_input=False, reduction='mean')(input, y_e)\n",
        "\n",
        "                # Compute gradient and ensure it's scalar\n",
        "                grad_e = grad(error_e, self.w, create_graph=True)[0]\n",
        "                penalty_e = grad_e.pow(2).mean()\n",
        "\n",
        "                total_penalty += penalty_e\n",
        "                total_error += error_e\n",
        "\n",
        "            loss = reg * total_error + (1 - reg) * total_penalty\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if args.verbose and iteration % 100 == 0:\n",
        "                w_str = pretty(self.solution())\n",
        "                print(\n",
        "                    \"{:05d} | {:.5f} | {:.5f} | {:.5f} | {}\".format(\n",
        "                        iteration, reg, total_error.item(), total_penalty.item(), w_str\n",
        "                    ))\n",
        "\n",
        "    def solution(self):\n",
        "        \"\"\"\n",
        "        Returns the learned model parameters as a numpy array.\n",
        "        \"\"\"\n",
        "        return (self.phi @ self.w).view(-1, 1).detach().numpy()\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        if isinstance(X, np.ndarray):\n",
        "            X = torch.tensor(X, dtype=torch.float32)\n",
        "\n",
        "        # Compute the logits using the learned parameters\n",
        "        pred = X @ torch.tensor(self.solution(), dtype=torch.float32)\n",
        "        rates = torch.exp(pred)\n",
        "\n",
        "        # Return the predicted Poisson rates (lambda)\n",
        "        return rates.detach().numpy()"
      ],
      "metadata": {
        "id": "t1mvS1wJmmAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Maximum Mean Discrepancy (MMD) Class Implementation\n",
        "\n",
        "This cell introduces the `MaximumMeanDiscrepancy` class, a domain generalization method that minimizes the distributional discrepancy between environments. The MMD algorithm uses a kernel-based measure to ensure that learned representations are invariant across environments.\n",
        "\n",
        "#### Key Components:\n",
        "1. **Initialization**:\n",
        "   - Converts input features (`exog`), targets (`endog`), and weights (`freqs`) to PyTorch tensors.\n",
        "   - Iteratively trains the model for various values of the regularization parameter (`gamma`) to select the best-performing model based on validation error.\n",
        "\n",
        "2. **Train Method**:\n",
        "   - Trains the MMD model by minimizing a weighted Poisson negative log-likelihood loss and the MMD penalty.\n",
        "   - Computes pairwise discrepancies between environments using the specified kernel type (e.g., Gaussian, mean covariance).\n",
        "\n",
        "3. **Solution**:\n",
        "   - Provides the learned model parameters in a usable format (NumPy array).\n",
        "\n",
        "4. **MMD Calculation**:\n",
        "   - Includes two types of kernel computations:\n",
        "     - **Gaussian Kernel**: Uses multiple bandwidths for robust similarity computation.\n",
        "     - **Mean-Covariance Kernel**: Considers both mean and covariance differences between distributions.\n",
        "\n",
        "5. **Prediction**:\n",
        "   - Uses the learned parameters to predict Poisson rates (`lambda`) for a given input.\n",
        "\n",
        "#### Parameters:\n",
        "- `exog`: Exogenous variables (input features).\n",
        "- `endog`: Endogenous variables (target values).\n",
        "- `freqs`: Weights associated with each environment.\n",
        "- `args`: Dictionary of hyperparameters including learning rate, number of iterations, and kernel type.\n",
        "\n",
        "#### Notable Features:\n",
        "- Supports various kernel types for measuring discrepancies between environments.\n",
        "- Implements a custom pairwise distance computation (`my_cdist`) for Gaussian kernels.\n",
        "- Integrates regularization to balance predictive accuracy and distributional alignment.\n",
        "\n",
        "This class enables robust training of models that generalize well across different environments by explicitly minimizing inter-environment discrepancies.\n"
      ],
      "metadata": {
        "id": "G-BntRsYrRrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaximumMeanDiscrepancy(object):\n",
        "    def __init__(self, exog, endog, freqs, args):\n",
        "        best_reg = 0\n",
        "        best_err = 1e6\n",
        "\n",
        "        exog = torch.tensor(np.array(exog), dtype=torch.float32)\n",
        "        endog = torch.tensor(np.array(endog), dtype=torch.float32)\n",
        "        freqs = torch.tensor(np.array(freqs), dtype=torch.float32)\n",
        "\n",
        "        x_val = exog[-1]\n",
        "        y_val = endog[-1]\n",
        "\n",
        "        for gamma in [1e-3, 1e-2, 1e-1, 1]:\n",
        "            self.train(exog[:-1], endog[:-1], freqs[:-1], args, gamma=gamma)\n",
        "            err = torch.mean((torch.exp(x_val @ self.solution()) - y_val) ** 2).item()\n",
        "\n",
        "            if args.verbose:\n",
        "                print(\"MMD (gamma={:.3f}) has {:.3f} validation error.\".format(gamma, err))\n",
        "\n",
        "            if err < best_err:\n",
        "                best_err = err\n",
        "                best_gamma = gamma\n",
        "                best_phi = self.phi.clone()\n",
        "        # Save the best model parameters\n",
        "        #self.phi = best_phi\n",
        "\n",
        "    def train(self, exog, endog, freqs, args, gamma=0):\n",
        "        dim_x = exog[0].size(1)\n",
        "        num_envs = exog.shape[0]\n",
        "\n",
        "        # Initialize phi and weights\n",
        "        self.phi = nn.Parameter(torch.empty(dim_x, dim_x))\n",
        "        nn.init.xavier_uniform_(self.phi)\n",
        "        self.w = torch.empty(dim_x, 1)\n",
        "        nn.init.xavier_uniform_(self.w)\n",
        "        self.w.requires_grad = True\n",
        "\n",
        "        # Optimizer and loss function for Poisson regression\n",
        "        opt = torch.optim.Adam([self.phi, self.w], lr=args.lr, weight_decay=1e-5)\n",
        "        #poisson_loss = torch.nn.PoissonNLLLoss(log_input=False, reduction='mean')\n",
        "\n",
        "        for iteration in range(args.n_iterations):\n",
        "            penalty = 0\n",
        "            error = 0\n",
        "\n",
        "            # Compute error and MMD penalty for each pair of environments\n",
        "            for i, (x_e1, y_e1, f) in enumerate(zip(exog, endog, freqs)):\n",
        "                input = torch.exp(x_e1 @ self.phi @ self.w)\n",
        "                error += weighted_poisson_nll(input, y_e1, f) # * f\n",
        "\n",
        "                for j, (x_e2, y_e2, f) in enumerate(zip(exog, endog, freqs)):\n",
        "                    if i < j:\n",
        "                        penalty += self.mmd(x_e1 @ self.phi, x_e2 @ self.phi, args.kernel_type) #* f\n",
        "\n",
        "            # Normalize penalty\n",
        "            error /= num_envs\n",
        "            if num_envs > 1:\n",
        "                penalty /= (num_envs * (num_envs - 1) / 2)\n",
        "\n",
        "            # Optimize the combined loss\n",
        "            opt.zero_grad()\n",
        "            loss = error + gamma * penalty\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            if args.verbose and iteration % 100 == 0:\n",
        "                w_str = pretty(self.solution())\n",
        "                print(\n",
        "                    \"{:05d} | {:.5f} | {:.3f} | {:.3f} | {}\".format(\n",
        "                        iteration, gamma, error, penalty, w_str\n",
        "                    ))\n",
        "\n",
        "    def solution(self):\n",
        "        \"\"\"Returns the learned model parameters as a numpy array.\"\"\"\n",
        "        return (self.phi @ self.w).view(-1, 1).detach().numpy()\n",
        "\n",
        "    def mmd(self, x, y, kernel_type):\n",
        "        if kernel_type == \"gaussian\":\n",
        "            Kxx = self.gaussian_kernel(x, x).mean()\n",
        "            Kyy = self.gaussian_kernel(y, y).mean()\n",
        "            Kxy = self.gaussian_kernel(x, y).mean()\n",
        "            return Kxx + Kyy - 2 * Kxy\n",
        "\n",
        "        elif kernel_type == \"mean_cov\":\n",
        "            mean_x = x.mean(0, keepdim=True)\n",
        "            mean_y = y.mean(0, keepdim=True)\n",
        "            cent_x = x - mean_x\n",
        "            cent_y = y - mean_y\n",
        "            cova_x = (cent_x.t() @ cent_x) / (len(x) - 1)\n",
        "            cova_y = (cent_y.t() @ cent_y) / (len(y) - 1)\n",
        "\n",
        "            mean_diff = (mean_x - mean_y).pow(2).mean()\n",
        "            cova_diff = (cova_x - cova_y).pow(2).mean()\n",
        "\n",
        "            return mean_diff + cova_diff\n",
        "        else:\n",
        "            raise ValueError(\"Unknown kernel type\")\n",
        "\n",
        "    def gaussian_kernel(self, x, y, gamma=[0.001, 0.01, 0.1, 1, 10, 100]):\n",
        "        D = self.my_cdist(x, y)\n",
        "        K = torch.zeros_like(D)\n",
        "        for g in gamma:\n",
        "            K.add_(torch.exp(D.mul(-g)))\n",
        "        return K\n",
        "\n",
        "    def my_cdist(self, x1, x2):\n",
        "        x1_norm = x1.pow(2).sum(dim=-1, keepdim=True)\n",
        "        x2_norm = x2.pow(2).sum(dim=-1, keepdim=True)\n",
        "        return torch.addmm(\n",
        "            x2_norm.transpose(-2, -1), x1, x2.transpose(-2, -1), alpha=-2\n",
        "        ).add_(x1_norm).clamp_min_(1e-30)\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        if isinstance(X, np.ndarray):\n",
        "            X = torch.tensor(X, dtype=torch.float32)\n",
        "\n",
        "        sol = torch.tensor(self.solution(), dtype=torch.float32)   # Flatten the solution\n",
        "        pred = X @ sol\n",
        "        rates = torch.exp(pred)\n",
        "        return rates.detach().numpy()  # Apply exponential for Poisson"
      ],
      "metadata": {
        "id": "-mggQ9Sdmm42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Empirical Risk Minimizer (ERM) Class Implementation\n",
        "\n",
        "This cell implements the `EmpiricalRiskMinimizer` class, which represents a baseline approach for domain generalization. ERM aggregates all data across environments to train a single model, ignoring potential differences between environments.\n",
        "\n",
        "#### Key Components:\n",
        "1. **Initialization**:\n",
        "   - Combines input features (`exog`), targets (`endog`), and weights (`freqs`) from all environments.\n",
        "   - Trains a Poisson regression model using scikit-learn's `PoissonRegressor` with sample weighting to account for varying frequencies across environments.\n",
        "\n",
        "2. **Model Training**:\n",
        "   - The regression model learns a single set of parameters (`w`) across all environments by minimizing the empirical risk (negative log-likelihood for Poisson regression).\n",
        "\n",
        "3. **Prediction**:\n",
        "   - Provides predictions for new input data (`X`) using the trained regression model.\n",
        "\n",
        "4. **Solution**:\n",
        "   - Exposes the learned model parameters (`w`) for further analysis or interpretation.\n",
        "\n",
        "#### Parameters:\n",
        "- `exog`: List of exogenous variables (covariates) for each environment.\n",
        "- `endog`: List of endogenous variables (target values) for each environment.\n",
        "- `freqs`: List of weights or frequencies for each environment.\n",
        "- `args`: Hyperparameters, including the number of iterations for model training.\n",
        "\n",
        "#### Use Case:\n",
        "ERM is a straightforward approach for modeling data when the goal is to optimize predictive performance on an aggregated dataset, without explicitly considering domain-specific characteristics.\n",
        "\n",
        "This implementation highlights the importance of baseline methods in evaluating advanced domain generalization algorithms.\n"
      ],
      "metadata": {
        "id": "AUjz6oNnrekC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmpiricalRiskMinimizer(object):\n",
        "    def __init__(self, exog, endog, freqs, args):\n",
        "        # x is the covariates matrix\n",
        "        # y is the depedent variable\n",
        "        x_all = np.concatenate([e for e in exog])\n",
        "        y_all = np.concatenate([e for e in endog])\n",
        "        freq_all = np.concatenate([e for e in freqs])\n",
        "\n",
        "        # w = LinearRegression(fit_intercept=False).fit(x_all, y_all).coef_\n",
        "        self.model = PoissonRegressor(max_iter=args.n_iterations, verbose=0)\n",
        "        self.model.fit(x_all, y_all.ravel(), sample_weight=freq_all.ravel())\n",
        "        w = self.model.coef_\n",
        "        self.w = w.reshape(-1, 1)\n",
        "\n",
        "    def predict(self, X):\n",
        "        res = self.model.predict(X)\n",
        "        return res\n",
        "\n",
        "    def solution(self):\n",
        "        return self.w"
      ],
      "metadata": {
        "id": "My8XvK2bmrW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Synthetic Data Generation"
      ],
      "metadata": {
        "id": "10pdpn9Nrzj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define input parameters"
      ],
      "metadata": {
        "id": "qMoIPwnUHMz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verbose = 1\n",
        "lr = 0.1\n",
        "n_iterations = 1000\n",
        "method = \"IRM\"\n",
        "covariate_dim = 10\n",
        "true_mu = 0.8\n",
        "emiter = 100\n",
        "kernel_type = \"gaussian\"\n",
        "start_alpha = 6.0\n",
        "start_beta = 4.0\n",
        "start_mu = 0.6"
      ],
      "metadata": {
        "id": "oTzVvuJpGwlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Synthetic Data Generator with Hawkes Process\n",
        "\n",
        "The class creates multiple environments with causal and acausal covariates and simulates events using a Hawkes process. Here's an overview:\n",
        "\n",
        "- **Causal and Acausal Covariates**: Covariates are generated for each environment, with causal relationships modeled explicitly.\n",
        "- **Hawkes Process**: Events are simulated using a discrete-time Hawkes process with a Weibull decay kernel, capturing time-dependent dependencies.\n",
        "- **Environment Simulation**: Multiple environments are constructed with varying noise levels, and event counts are generated for each.\n",
        "\n",
        "The `SyntheticEnvs` class provides methods to:\n",
        "1. Generate causal and acausal covariates.\n",
        "2. Simulate events using a Hawkes process.\n",
        "3. Compute the \"true\" underlying solution for the synthetic dataset.\n"
      ],
      "metadata": {
        "id": "ClDaFYh4eJwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SyntheticEnvs:\n",
        "    def __init__(self, n_cty, n_day, alpha, beta, mu, env_list, dim, h):\n",
        "        \"\"\"\n",
        "        Initializes variables for synthetic data generation\n",
        "        assume n_cty is number of samples\n",
        "        Days ,alpha, beta, mu, environment values, covariate dimension\n",
        "        h is the seed for the random number generator\n",
        "        covariates, weights generated here\n",
        "        Only one layer of dimensionality, mu and cov constant across days\n",
        "        \"\"\"\n",
        "        # Set seeds for all possible sources of randomness\n",
        "        self.h = h\n",
        "        np.random.seed(self.h)\n",
        "        random.seed(self.h)\n",
        "        os.environ['PYTHONHASHSEED'] = str(self.h)\n",
        "\n",
        "        self.n_cty = n_cty\n",
        "        self.n_day = n_day\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.mu = mu\n",
        "\n",
        "        # generate events w mob demo data for 1 env\n",
        "        self.dim_x = dim // 2\n",
        "        self.causal_cov = np.random.rand(self.n_cty, self.dim_x)\n",
        "        if self.dim_x == 4:\n",
        "            self.wxy = np.array([\n",
        "                [1, -2, 3, 0],\n",
        "                [-0.34, 2, 0, 0.5],\n",
        "                [0, -2, -1, 0],\n",
        "                [0, 1, -2.5, -2]\n",
        "            ])\n",
        "        else:\n",
        "            self.wxy = np.random.rand(self.dim_x, self.dim_x) * 0.15 # scaled to [0, 0.25] range\n",
        "\n",
        "        # generate acausal data and events for all envs\n",
        "        covariate_groups, case_count_groups, lambda_groups = self.makeEnvs(env_list)\n",
        "\n",
        "        self.case_count_envs = case_count_groups\n",
        "        self.covariate_envs = covariate_groups\n",
        "        self.lambda_envs = lambda_groups\n",
        "\n",
        "    def hawkes_discrete_simulation(self, mu, R):\n",
        "        \"\"\"\n",
        "        Simulates a Hawkes process with discrete time steps using the thinning method.\n",
        "        generates exponentially decaying continuous time steps, then calculates probability\n",
        "        of acceptance using discretized binned timestep\n",
        "\n",
        "        Lambda = mu + Sum{R(X0)w(t-t-j) : t_j<t}\n",
        "        Parameters:\n",
        "            mu (numpy array): Background rate\n",
        "            R (numpy array): Reproductive rate (shape: n_cty, 1).\n",
        "            xw (numpy array): Covariates\n",
        "\n",
        "        Returns:\n",
        "            events (numpy array): event counts for each county (shape: n_cty, T).\n",
        "            lambda_t (numpy array): conditional intensity values (shape: n_cty, T).\n",
        "        \"\"\"\n",
        "        T = self.n_day  # time steps\n",
        "\n",
        "        # Estimate lambda_max using Weibull maximum\n",
        "        t_peak = self.alpha * ((self.beta - 1) / self.beta) ** (1 / self.beta) if self.beta > 1 else 0\n",
        "        w_peak = weibull_min.pdf(t_peak, self.beta, scale=self.alpha)\n",
        "        R_max = np.max(R)\n",
        "        lambda_max = np.max(mu) + np.sum(R_max * w_peak)\n",
        "\n",
        "        # outputs\n",
        "        events = np.zeros((self.n_cty, T), dtype=int)\n",
        "        lambda_t = np.zeros((self.n_cty, T))\n",
        "\n",
        "        # samples events n_cty times for n_cty event rates\n",
        "        for i in range(self.n_cty):\n",
        "            t = 0\n",
        "            # Add safety counter to prevent infinite loops\n",
        "            iteration_count = 0\n",
        "            max_iterations = 1000000  # Adjust this number as needed\n",
        "\n",
        "            while t < T:\n",
        "                # Add safety check\n",
        "                iteration_count += 1\n",
        "                if iteration_count > max_iterations:\n",
        "                    print(f\"Warning: Maximum iterations reached for county {i}\")\n",
        "                    break\n",
        "\n",
        "                delta_t = np.random.exponential(1 / lambda_max)\n",
        "                t_candidate = t + delta_t\n",
        "\n",
        "                # Add minimum time step to ensure progress\n",
        "                if delta_t < 1e-10:  # Prevent extremely small time steps\n",
        "                    delta_t = 1e-10\n",
        "                    t_candidate = t + delta_t\n",
        "\n",
        "                if t_candidate >= T:\n",
        "                    break\n",
        "\n",
        "                t_discrete = int(np.floor(t_candidate))\n",
        "\n",
        "                # triggering kernel influence\n",
        "                past_events = np.where(events[i, :t_discrete] > 0)[0]\n",
        "                hist_influence = np.sum([\n",
        "                    R[i] * weibull_min.pdf(t_discrete - t_j, self.alpha, loc=0, scale=self.beta)\n",
        "                    for t_j in past_events\n",
        "                ])\n",
        "                lambda_t_candidate = mu[i] + hist_influence\n",
        "\n",
        "                if np.random.uniform(0, 1) <= (lambda_t_candidate / lambda_max):\n",
        "                    events[i, t_discrete] += 1\n",
        "\n",
        "                t = t_candidate\n",
        "\n",
        "            # Compute lambda_t for all time steps\n",
        "            for t in range(T):\n",
        "                past_events = np.where(events[i, :t] > 0)[0]\n",
        "                hist_influence = np.sum([\n",
        "                    R[i] * weibull_min.pdf(t - t_j, self.beta, scale=self.alpha)\n",
        "                    for t_j in past_events\n",
        "                ])\n",
        "                lambda_t[i, t] = mu[i] + hist_influence\n",
        "            print(f\"Sampling complete for county {i}\")\n",
        "\n",
        "        return events, lambda_t\n",
        "\n",
        "    def trueSolution(self):\n",
        "        w_reduced = np.sum(self.wxy, axis=1)\n",
        "        sol = np.concatenate([w_reduced, np.zeros(self.dim_x)])\n",
        "        # shape of sol is (2dim_x, 1); dim_x is number of causal covariates\n",
        "        return sol\n",
        "\n",
        "    def makeEnvs(self, env_list):\n",
        "        \"\"\"\n",
        "        Makes a set of environments using an input env_list\n",
        "        one set of acausal cov geenrated with env dependent noise for each env\n",
        "        X: causal and acausal cov concatenated\n",
        "        Y: y summed\n",
        "        (X, Y) returned\n",
        "        after which R will be poisson sampled from Y\n",
        "        and case count will be generated from R, mu(same for all envs)\n",
        "        init shuld return sets of case count and X for each env (observed variables for EM)\n",
        "        \"\"\"\n",
        "        x = self.causal_cov # shape (n_cty, dim_x)\n",
        "        y = x @ self.wxy\n",
        "        wyz = np.random.rand(self.dim_x, self.dim_x)\n",
        "        all_covariates = []\n",
        "        all_lambdas = []\n",
        "        for _, e in enumerate(env_list):\n",
        "            z_e = y @ wyz + np.random.rand(self.n_cty, self.dim_x) * e\n",
        "            cov = np.concatenate([x, z_e], axis=1)  # Concatenate vectors\n",
        "            all_covariates.append(cov)\n",
        "\n",
        "        w = np.sum(self.wxy, axis=1, keepdims=True)\n",
        "        R = np.exp(x @ w).flatten() # shape (n_cty, 1)\n",
        "\n",
        "        all_case_counts = []\n",
        "        for e in range(len(env_list)):\n",
        "            # generates event sequence for each environment\n",
        "            print(f\"Starting simulation for env {e}\")\n",
        "            case_count_e, lambda_e = self.hawkes_discrete_simulation(self.mu, R)\n",
        "            all_case_counts.append(case_count_e)\n",
        "            all_lambdas.append(lambda_e)\n",
        "\n",
        "        return all_covariates, all_case_counts, all_lambdas"
      ],
      "metadata": {
        "id": "zSH3HkyEnEZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Synthetic data parameters"
      ],
      "metadata": {
        "id": "WhG2g5GqoV4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_day = 20\n",
        "true_alpha = 1.0\n",
        "true_beta = 1.81\n",
        "env_list = [1.2, 1.8, 3, 0.2, 5]\n",
        "covariate_dim = 10\n",
        "true_mu = 0.8\n",
        "n_cty = 20\n",
        "seed = 138"
      ],
      "metadata": {
        "id": "ekcqXSR2oUni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mu = np.repeat(true_mu, n_cty)\n",
        "True_Data = SyntheticEnvs(\n",
        "    n_cty,\n",
        "    n_day,\n",
        "    true_alpha,\n",
        "    true_beta,\n",
        "    mu,\n",
        "    env_list,\n",
        "    covariate_dim,\n",
        "    seed\n",
        ")"
      ],
      "metadata": {
        "id": "tqt1ownOoufk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    'case_count_all': True_Data.case_count_envs,\n",
        "    'covariates_all': True_Data.covariate_envs,\n",
        "    'true_weights': True_Data.trueSolution(),\n",
        "    'true_lambdas': True_Data.lambda_envs,\n",
        "    'true_params': {\n",
        "        'true_alpha': true_alpha,\n",
        "        'true_beta': true_beta,\n",
        "        'true_mu': mu,\n",
        "        'seed': seed,\n",
        "        'env_list': env_list,\n",
        "        'n_cty': n_cty,\n",
        "        'n_day': n_day\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "rxoa3eMpoyc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualise synthetic data"
      ],
      "metadata": {
        "id": "jOYhuNTMsqVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 1: Lambda heatmaps\n",
        "n_envs = len(data['true_lambdas'])\n",
        "n_cols = min(3, n_envs)\n",
        "n_rows = (n_envs + 2) // 3\n",
        "\n",
        "fig1, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))\n",
        "if n_envs > 1:\n",
        "    axes = axes.flatten()\n",
        "else:\n",
        "    axes = [axes]\n",
        "\n",
        "for i, lambda_env in enumerate(data['true_lambdas']):\n",
        "    sns.heatmap(lambda_env, ax=axes[i], cmap='YlOrRd')\n",
        "    axes[i].set_title(f'Environment {i + 1}')\n",
        "    axes[i].set_xlabel('Time')\n",
        "    axes[i].set_ylabel('Location')\n",
        "\n",
        "# Add parameter values as text at the bottom of the figure\n",
        "param_text = f'μ = {true_mu:.2f}, α = {true_alpha:.2f}, β = {true_beta:.2f}'\n",
        "fig1.text(0.5, 0.02, param_text, ha='center', fontsize=10)\n",
        "\n",
        "# Adjust layout to make room for the parameter text\n",
        "plt.tight_layout(rect=[0, 0.05, 1, 1])\n",
        "\n",
        "for j in range(i + 1, len(axes)):\n",
        "    axes[j].remove()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Plot 2: Cumulative case counts\n",
        "fig2, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "colors = plt.cm.rainbow(np.linspace(0, 1, n_envs))\n",
        "for i, cases in enumerate(data['case_count_all']):\n",
        "    cumulative_cases = np.cumsum(cases.sum(axis=0))\n",
        "    ax.plot(cumulative_cases, color=colors[i],\n",
        "            label=f'Environment {i + 1}', linewidth=2)\n",
        "\n",
        "ax.set_xlabel('Time')\n",
        "ax.set_ylabel('Cumulative Cases')\n",
        "ax.set_title('Cumulative Case Counts Across Environments')\n",
        "ax.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Plot 3: Daily new cases\n",
        "fig3, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "for i, cases in enumerate(data['case_count_all']):\n",
        "    daily_cases = cases.sum(axis=0)  # Sum across all locations for each day\n",
        "    ax.plot(daily_cases, color=colors[i],\n",
        "            label=f'Environment {i + 1}', linewidth=2)\n",
        "\n",
        "ax.set_xlabel('Time')\n",
        "ax.set_ylabel('Daily New Cases')\n",
        "ax.set_title('Daily New Cases Across Environments')\n",
        "ax.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kcmH88aApGoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "VQLwF1SveQt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "case_count_all = data['case_count_all']\n",
        "covariates_all = data['covariates_all']\n",
        "true_weights = data['true_weights']\n",
        "true_lambdas = data['true_lambdas']"
      ],
      "metadata": {
        "id": "ySy7Oks8eM5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "IvdCazHGs3SI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "True Parameters"
      ],
      "metadata": {
        "id": "5EDui9HveSS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args.true_alpha = data['true_params']['true_alpha']\n",
        "args.true_beta = data['true_params']['true_beta']\n",
        "args.n_day = data['true_params']['n_day']\n",
        "args.n_cty = data['true_params']['n_cty']\n",
        "args.env_list = data['true_params']['env_list']\n",
        "mu = data['true_params']['true_mu']"
      ],
      "metadata": {
        "id": "EQqyTLvJeUbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial guesses for Hawkes process"
      ],
      "metadata": {
        "id": "PI0p2Ychhjce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_data = [\n",
        "    {\n",
        "        \"R0\": np.exp(covariates_all[e] @ np.random.rand(args.covariate_dim)).reshape(-1, 1),\n",
        "        \"mus\": np.full((n_cty, 1), args.start_mu),\n",
        "        \"lambda_t\": np.zeros((n_cty, n_day)),\n",
        "        \"p_c_ij\": None,\n",
        "        \"p_c_ii\": None,\n",
        "        \"Q\": None,\n",
        "        \"alpha\": args.start_alpha,\n",
        "        \"beta\": args.start_beta\n",
        "    } for e in range(n_env)\n",
        "]"
      ],
      "metadata": {
        "id": "PJrpYksKhid1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tracking updates across iterations"
      ],
      "metadata": {
        "id": "LQk_QOqch_M7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mus_prev, theta_prev = [None] * n_env, [None] * n_env\n",
        "alphas_prev = [None] * n_env\n",
        "betas_prev = [None] * n_env\n",
        "alpha_delta = [[] for _ in range(n_env)]\n",
        "beta_delta = [[] for _ in range(n_env)]\n",
        "mus_delta = [[] for _ in range(n_env)]\n",
        "theta_delta = [[] for _ in range(n_env)]"
      ],
      "metadata": {
        "id": "i57tBoqPiBoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "combined evironemnts for standardization"
      ],
      "metadata": {
        "id": "e_mKPuLciVIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "covariates_full = np.vstack([covariates_all[e] for e in range(n_env)])\n",
        "cov_mean = np.mean(covariates_full, axis=0)\n",
        "cov_std = np.std(covariates_full, axis=0)\n",
        "# Add small constant to avoid division by zero\n",
        "cov_std = np.where(cov_std < 1e-10, 1e-10, cov_std)\n",
        "# Standardize each environment using the global parameters\n",
        "for e in range(n_env):\n",
        "    covariates_all[e] = (covariates_all[e] - cov_mean) / cov_std"
      ],
      "metadata": {
        "id": "zFucBmjPiUzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# E M Algorithm"
      ],
      "metadata": {
        "id": "ebQ8cshuim_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for itr in range(emitr):\n",
        "    start_time = time.time()\n",
        "    print(\"----------------------------------------------------------\")\n",
        "    print(f\"Starting EM Iteration: {itr+1}\")\n",
        "\n",
        "    for e in range(n_env):\n",
        "        # E-step\n",
        "        T = (case_count_all[e].shape[1])\n",
        "        R0_ext_j = np.repeat(env_data[e][\"R0\"], T, axis=0) # shape: n_cty*n_day, 1\n",
        "        trig_comp = R0_ext_j * wblval(T, n_cty, env_data[e][\"alpha\"], 0, env_data[e][\"beta\"]) * (np.repeat(case_count_all[e], T, axis=0) > 0)\n",
        "        mu_comp = np.tile(np.eye(T), (n_cty, 1)) * np.repeat(env_data[e][\"mus\"], T, axis=0)\n",
        "        lambda_t = np.sum(mu_comp + trig_comp, axis=1, keepdims=True)\n",
        "        env_data[e][\"lambda_t\"] = lambda_t\n",
        "        env_data[e][\"p_c_ij\"] = np.divide(trig_comp, lambda_t, where= lambda_t != 0)\n",
        "        env_data[e][\"p_c_ii\"] = np.divide(mu_comp, lambda_t, where= lambda_t != 0)\n",
        "\n",
        "        P_c_j = env_data[e][\"p_c_ij\"].reshape(n_day, n_day*n_cty)\n",
        "        P_c_j = np.reshape(np.sum(P_c_j, axis=0), (n_cty, n_day))\n",
        "        Q = P_c_j #* case_count_all[e]\n",
        "        env_data[e][\"Q\"] = Q\n",
        "\n",
        "        print(f\"for environment {e}: \")\n",
        "        print(f\"Background rate spans: {np.min(env_data[e]['mus']):.4f} to {np.max(env_data[e]['mus']):.4f}\")\n",
        "        print(f\"Conditional intensity: {np.min(env_data[e]['lambda_t']):.4f} to {np.max(env_data[e]['lambda_t']):.4f}\")\n",
        "        print(f\"Q- Average children: {np.min(env_data[e]['Q']):.4f} to {np.max(env_data[e]['Q']):.4f}\")\n",
        "        print(f\"P_c(i,j) range: {np.min(env_data[e]['p_c_ij']):.4f} to {np.max(env_data[e]['p_c_ij']):.4f}\")\n",
        "        print(f\"P_c(i,i) range: {np.min(env_data[e]['p_c_ii']):.4f} to {np.max(env_data[e]['p_c_ii']):.4f}\")\n",
        "\n",
        "    endog = [env[\"Q\"].reshape(-1,1) for env in env_data]\n",
        "    exog = [np.repeat(covariates_all[e], T, axis=0) for e in range(n_env)]\n",
        "    event_freqs = [case_count_all[e].reshape(-1,1) for e in range(n_env)]\n",
        "    # not using frequencies for now\n",
        "    if args.verbose:\n",
        "        print(f\"Starting poisson model learning using Domain Generalization: {args.method} \")\n",
        "    if args.method == \"IRM\":\n",
        "        model = InvariantRiskMinimization(exog, endog, event_freqs, args)\n",
        "\n",
        "    elif args.method == \"ERM\":\n",
        "        model = EmpiricalRiskMinimizer(exog, endog, event_freqs, args)\n",
        "\n",
        "    elif args.method == \"MMD\":\n",
        "        model = MaximumMeanDiscrepancy(exog, endog, event_freqs, args)\n",
        "\n",
        "    coef = model.solution()\n",
        "    print(f\"Estimated coeffecients across all environments in EM itr: {itr}:\")\n",
        "    print(coef)\n",
        "\n",
        "    for e in range(n_env):\n",
        "        # Update R0 with predictions for M-step\n",
        "        env_data[e][\"R0\"] = model.predict(covariates_all[e])\n",
        "        env_data[e][\"R0\"] = np.reshape(env_data[e][\"R0\"], (-1,1))\n",
        "\n",
        "        # M-step\n",
        "        mus = np.sum(env_data[e][\"p_c_ii\"], axis=1, keepdims=True)\n",
        "        mus = mus.reshape(n_cty, n_day)\n",
        "        mus = np.sum(mus, axis=1, keepdims=True) / n_day\n",
        "        mus = np.clip(mus, 0, 1.5)\n",
        "        env_data[e][\"mus\"] = mus\n",
        "\n",
        "        obs = np.tril(np.arange(1, n_day+1)[:, None] - np.arange(1, n_day+1))\n",
        "        weights = np.sum(env_data[e][\"p_c_ij\"].reshape(n_cty, n_day, n_day), axis=0)\n",
        "        obs_1d = np.arange(1, n_day + 1)\n",
        "        weights_1d = np.array([np.sum(weights[np.where(obs == time_diff)]) for time_diff in obs_1d])\n",
        "        normalized_weights = np.divide(weights_1d, np.sum(weights_1d), where=np.sum(weights_1d) != 0)\n",
        "        sample_size = 1000  # Choose an appropriate sample size\n",
        "        sampled_times = np.random.choice(obs_1d, size=sample_size, p=normalized_weights)\n",
        "        shape, loc, scale = weibull_min.fit(sampled_times, floc=0)\n",
        "        env_data[e][\"beta\"] = np.clip(scale, 0.5, 20)\n",
        "        env_data[e][\"alpha\"] = np.clip(shape, 0.5, 25)\n",
        "\n",
        "        print(f\"Environment {e+1}: Mus from {np.min(env_data[e]['mus']):.4f} to {np.max(env_data[e]['mus']):.4f}\")\n",
        "        print(f\"Weibull scale: {env_data[e]['alpha']:.4f}, Shape: {env_data[e]['beta']:.4f}\")\n",
        "\n",
        "        if itr == 0:\n",
        "            # Save the first iteration values\n",
        "            mus_prev[e] = np.mean(env_data[e][\"mus\"], axis=0)\n",
        "            theta_prev[e] = coef.flatten()\n",
        "            alphas_prev[e] = env_data[e][\"alpha\"]\n",
        "            betas_prev[e] = env_data[e][\"beta\"]\n",
        "        else:\n",
        "            # Calculate RMSR for convergence check\n",
        "            mus_delta[e].append(np.sqrt(np.mean((mus_prev[e] - env_data[e][\"mus\"]) ** 2)))\n",
        "            theta_delta[e].append(np.sqrt(np.mean((theta_prev[e] - coef) ** 2)))\n",
        "            alpha_delta[e].append(np.sqrt((env_data[e][\"alpha\"] - alphas_prev[e]) ** 2))\n",
        "            beta_delta[e].append(np.sqrt((env_data[e][\"beta\"] - betas_prev[e]) ** 2))\n",
        "\n",
        "            # Save current values for next iteration\n",
        "            mus_prev[e] = env_data[e][\"mus\"]\n",
        "            theta_prev[e] = coef\n",
        "            alphas_prev[e] = env_data[e][\"alpha\"]\n",
        "            betas_prev[e] = env_data[e][\"beta\"]\n",
        "\n",
        "    # Early stopping criteria\n",
        "    if itr > 5:\n",
        "        print(\"Commencing convergence check: \")\n",
        "        converged = True\n",
        "        for e in range(n_env):\n",
        "            env_converged = (\n",
        "                np.all(np.array(mus_delta[e][-5:]) < break_diff) and\n",
        "                np.all(np.array(theta_delta[e][-5:]) < break_diff) and\n",
        "                np.all(np.array(alpha_delta[e][-5:]) < break_diff) and\n",
        "                np.all(np.array(beta_delta[e][-5:]) < break_diff)\n",
        "            )\n",
        "            if not env_converged:\n",
        "                converged = False\n",
        "                break\n",
        "\n",
        "        if converged:\n",
        "            print(f\"Convergence criterion met at iteration {itr + 1}. Exiting EM loop.\")\n",
        "\n",
        "            # Save all plots to converged directory\n",
        "            # Convergence plot\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(np.arange(1, len(alpha_delta[0])+1), np.mean(alpha_delta, axis=0), label=\"Alpha Deltas\", color=\"red\")\n",
        "            plt.plot(np.arange(1, len(beta_delta[0])+1), np.mean(beta_delta, axis=0), label=\"Beta Deltas\", color=\"blue\")\n",
        "            plt.plot(np.arange(1, len(mus_delta[0])+1), np.mean(mus_delta, axis=0), label=\"Mus Deltas\", color=\"green\")\n",
        "            plt.plot(np.arange(1, len(theta_delta[0])+1), np.mean(theta_delta, axis=0), label=\"0 Deltas\", color=\"orange\")\n",
        "            plt.legend()\n",
        "            plt.savefig(os.path.join(converged_dir, \"convergence_plot.png\"))\n",
        "            plt.close()\n",
        "\n",
        "            # Final comparison plots\n",
        "            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(converged_dir, \"final_comparison_plots.png\"))\n",
        "            plt.close()\n",
        "\n",
        "            # Save model parameters\n",
        "            avg_alpha = np.mean([env[\"alpha\"] for env in env_data])\n",
        "            avg_beta = np.mean([env[\"beta\"] for env in env_data])\n",
        "            avg_mus = np.mean([env[\"mus\"] for env in env_data], axis=0)\n",
        "            scaled_coef = coef.flatten() * cov_std\n",
        "\n",
        "            model_params = {\n",
        "                'method': args.method,\n",
        "                'true_params': {\n",
        "                    'alpha': args.true_alpha,\n",
        "                    'beta': args.true_beta,\n",
        "                    'mu': mu.flatten().tolist(),\n",
        "                    'weights': true_weights.flatten().tolist()\n",
        "                },\n",
        "                'estimated_params': {\n",
        "                    'alpha_per_env': [env[\"alpha\"] for env in env_data],\n",
        "                    'beta_per_env': [env[\"beta\"] for env in env_data],\n",
        "                    'mu_per_env': [env[\"mus\"].flatten().tolist() for env in env_data],\n",
        "                    'avg_alpha': avg_alpha,\n",
        "                    'avg_beta': avg_beta,\n",
        "                    'avg_mus': avg_mus.flatten().tolist(),\n",
        "                    'scaled_coefficients': scaled_coef.tolist()\n",
        "                },\n",
        "                'convergence_metrics': {\n",
        "                    'final_iteration': itr + 1,\n",
        "                    'alpha_delta': np.mean(alpha_delta, axis=0).tolist(),\n",
        "                    'beta_delta': np.mean(beta_delta, axis=0).tolist(),\n",
        "                    'mus_delta': np.mean(mus_delta, axis=0).tolist(),\n",
        "                    'theta_delta': np.mean(theta_delta, axis=0).tolist()\n",
        "                },\n",
        "                'mse_metrics': {\n",
        "                    'alpha_mse': float((args.true_alpha - avg_alpha)**2),\n",
        "                    'beta_mse': float((args.true_beta - avg_beta)**2),\n",
        "                    'mus_mse': float(np.mean((mu.flatten() - avg_mus.flatten())**2)),\n",
        "                    'weights_mse': float(np.mean((true_weights - scaled_coef)**2))\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Save parameters to JSON file\n",
        "            with open(os.path.join(converged_dir, 'model_parameters.json'), 'w') as f:\n",
        "                json.dump(model_params, f, indent=4)\n",
        "\n",
        "            break\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Iteration {itr+1} completed in {elapsed_time:.2f} seconds.\")\n",
        "    print(\"----------------------------------------------------------\")\n",
        "\n",
        "\n",
        "print(\"EM Algorithm Completed.\")"
      ],
      "metadata": {
        "id": "25J0xgLRipSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Model parameters"
      ],
      "metadata": {
        "id": "fhfE8Qb_i9t5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_alpha = np.mean([env[\"alpha\"] for env in env_data])\n",
        "avg_beta = np.mean([env[\"beta\"] for env in env_data])\n",
        "avg_mus = np.mean([env[\"mus\"] for env in env_data], axis=0)\n",
        "scaled_coef = coef.flatten() * cov_std"
      ],
      "metadata": {
        "id": "kxWeWq0gjALA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Results compilation"
      ],
      "metadata": {
        "id": "wYNTudwgjOD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(1, len(alpha_delta[0])+1), np.mean(alpha_delta, axis=0), label=\"Alpha Deltas\", color=\"red\")\n",
        "plt.plot(np.arange(1, len(beta_delta[0])+1), np.mean(beta_delta, axis=0), label=\"Beta Deltas\", color=\"blue\")\n",
        "plt.plot(np.arange(1, len(mus_delta[0])+1), np.mean(mus_delta, axis=0), label=\"Mus Deltas\", color=\"green\")\n",
        "plt.plot(np.arange(1, len(theta_delta[0])+1), np.mean(theta_delta, axis=0), label=\"0 Deltas\", color=\"orange\")\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(checkpoint_dir, \"final_convergence_plot.png\"))\n",
        "plt.close()\n",
        "\n",
        "# Calculate averages across environments\n",
        "avg_alpha = np.mean([env[\"alpha\"] for env in env_data])\n",
        "avg_beta = np.mean([env[\"beta\"] for env in env_data])\n",
        "avg_mus = np.mean([env[\"mus\"] for env in env_data], axis=0)\n",
        "\n",
        "# Create a figure with 3 subplots\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Alpha and Beta comparison\n",
        "params = ['Alpha', 'Beta']\n",
        "true_vals = [args.true_alpha, args.true_beta]\n",
        "est_vals = [avg_alpha, avg_beta]\n",
        "x = np.arange(len(params))\n",
        "width = 0.35\n",
        "\n",
        "ax1.bar(x - width/2, true_vals, width, label='True Values', color='skyblue')\n",
        "ax1.bar(x + width/2, est_vals, width, label='Estimated Values', color='lightcoral')\n",
        "ax1.set_ylabel('Value')\n",
        "ax1.set_title('True vs Estimated Alpha and Beta')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(params)\n",
        "ax1.legend()\n",
        "\n",
        "# Plot 2: True mu vs Estimated mu comparison across counties\n",
        "x = np.arange(len(mu))\n",
        "ax2.bar(x - width/2, mu.flatten(), width, label='True μ', color='skyblue')\n",
        "ax2.bar(x + width/2, avg_mus.flatten(), width, label='Estimated μ', color='lightcoral')\n",
        "ax2.set_xlabel('County Index')\n",
        "ax2.set_ylabel('Background Rate (μ)')\n",
        "ax2.set_title('True vs Estimated Background Rates')\n",
        "ax2.set_xticks(x)\n",
        "ax2.legend()\n",
        "\n",
        "# Plot 3: True weights vs Estimated coefficients across dimensions\n",
        "x = np.arange(len(true_weights))\n",
        "scaled_coef = coef.flatten() * cov_std\n",
        "ax3.bar(x - width/2, true_weights.flatten(), width, label='True Weights', alpha=0.7)\n",
        "ax3.bar(x + width/2, scaled_coef, width, label='Estimated Coef', alpha=0.7)\n",
        "ax3.set_xlabel('Coefficient Index')\n",
        "ax3.set_ylabel('Value')\n",
        "ax3.set_title('True vs Estimated Weights')\n",
        "ax3.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(checkpoint_dir, \"final_comparison_plots.png\"))\n",
        "plt.close()\n",
        "\n",
        "# Print MSE values\n",
        "print(f\"MSE for Alpha: {(args.true_alpha - avg_alpha)**2:.4f}\")\n",
        "print(f\"MSE for Beta: {(args.true_beta - avg_beta)**2:.4f}\")\n",
        "print(f\"MSE for Mus: {np.mean((mu.flatten() - avg_mus.flatten())**2):.4f}\")\n",
        "print(f\"MSE for Weights: {np.mean((true_weights - scaled_coef)**2):.4f}\")\n",
        "\n",
        "print(\"\\nFinal Model Parameters:\")\n",
        "print(\"-----------------------\")\n",
        "print(f\"True Parameters:\")\n",
        "print(f\"- Alpha: {args.true_alpha:.4f}\")\n",
        "print(f\"- Beta: {args.true_beta:.4f}\")\n",
        "print(f\"- Mu: {mu.flatten()}\")\n",
        "print(f\"- Weights: {true_weights.flatten()}\")\n",
        "print(\"\\nEstimated Parameters:\")\n",
        "print(f\"- Alpha (avg across envs): {avg_alpha:.4f}\")\n",
        "print(f\"- Beta (avg across envs): {avg_beta:.4f}\")\n",
        "print(f\"- Mu (avg across envs): {avg_mus.flatten()}\")\n",
        "print(f\"- Weights (scaled): {scaled_coef}\")\n",
        "\n",
        "# Print per-environment parameters\n",
        "print(\"\\nPer-Environment Parameters:\")\n",
        "for e in range(len(env_data)):\n",
        "    print(f\"\\nEnvironment {e+1}:\")\n",
        "    print(f\"- Alpha: {env_data[e]['alpha']:.4f}\")\n",
        "    print(f\"- Beta: {env_data[e]['beta']:.4f}\")\n",
        "    print(f\"- Mu range: [{np.min(env_data[e]['mus']):.4f}, {np.max(env_data[e]['mus']):.4f}]\")"
      ],
      "metadata": {
        "id": "HneRSmmsjM2Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}